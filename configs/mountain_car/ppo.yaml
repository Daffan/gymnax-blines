train_config:
  train_type: "PPO"
  env_name: "MountainCar-v0"
  num_train_steps: 500000
  evaluate_every_epochs: 1000
  
  num_train_envs: 16
  max_grad_norm: 1.0
  gamma: 0.99  # Discount factor
  n_steps: 32 # "GAE n-steps"
  n_minibatch: 4 # "Number of PPO minibatches"
  lr_begin: 5e-03  # Start PPO learning rate
  lr_end: 5e-04 #  End PPO learning rate
  lr_warmup: 0.05 # Prop epochs until warmup is completed 
  epoch_ppo: 64  # "Number of PPO epochs on a single batch"
  clip_eps: 0.2 # "Clipping range"
  gae_lambda: 0.95 # "GAE lambda"
  entropy_coeff: 0.003 # "Entropy loss coefficient"
  critic_coeff: 0.5  # "Value loss coefficient"

  network_name: "Categorical-MLP"
  network_config:
    num_hidden_units: 64
    num_hidden_layers: 2

  problem_test_config:
    num_env_steps: 200
    num_rollouts: 164

log_config:
  time_to_track: ["num_steps"]
  what_to_track: ["return"]
  verbose: true
  print_every_k_updates: 1
  overwrite: 1
  model_type: "jax"

device_config:
  num_devices: 1
  device_type: "gpu"